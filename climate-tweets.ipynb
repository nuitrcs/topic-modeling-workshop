{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate change tweets from data world.\n",
    "#### Based on code by James at the Coding Club.\n",
    "#### Modified for Topic Modeling Workshop at Northwestern University, August, 2019.\n",
    "#### [https://github.com/nuitrcs/topic-modeling-workshop](https://github.com/nuitrcs/topic-modeling-workshop)\n",
    "\n",
    "In this example we'll be looking at a series of tweets\n",
    "discussing climate change issues. Tweets are short texts\n",
    "like the ABC News headlines. Unlike the headlines, Tweets\n",
    "contain a variety of special tags which we'll need to\n",
    "process.\n",
    "\n",
    "We'll use Non-negative matrix factorization to extract topics \n",
    "once we've preprocessed the tweets. \n",
    "\n",
    "Since tweets are short texts, we might expect NMF to perform \n",
    "better than LDA.  You'll be asked to check this by comparing \n",
    "the results obtained using Latent Dirichlet Allocation with those \n",
    "obtained using NMF.\n",
    "\n",
    "The input data consists of 6090 tweets plus a column header \"tweet\".\n",
    "\n",
    "Here are two sample tweets.  \n",
    "\n",
    "* RT @our_codingclub: Can @you find #all the #hashtags?\n",
    "* Not a retweet. All views @my own\n",
    "\n",
    "In these tweets:\n",
    "\n",
    "* *RT* indicates a retweet.\n",
    "* *@something* indicates \"something\" is a twitter handle.\n",
    "* *#hashtag* indicates a hashtag.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load climate tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( 'data/climate_tweets.csv' )\n",
    "\n",
    "# df (dataframe) now contains the text of the tweets, one per row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a new 'is_retweet' column to highlight retweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_retweet'] = df['tweet'].apply(lambda x: x[:2]=='RT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of retweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_retweet'].sum()  # number of retweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the number of unique retweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['is_retweet']].tweet.unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the ten most repeated tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['tweet']).size().reset_index(name='counts')\\\n",
    "  .sort_values('counts', ascending=False).head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count number of times each tweet appears.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby(['tweet']).size()\\\n",
    "           .reset_index(name='counts')\\\n",
    "           .counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define bins for histogram of counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bins = np.arange(0,counts.max()+2, 1)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a histogram of tweet counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(counts, bins = my_bins)\n",
    "plt.xlabels = np.arange(1,counts.max()+1, 1)\n",
    "plt.xlabel('copies of each tweet')\n",
    "plt.ylabel('frequency')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions to extract twitter handles and hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_retweeted(tweet):\n",
    "    '''Extract the twitter handles of retweeted people'''\n",
    "    return re.findall('(?<=RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    '''Extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.findall('(?<!RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)  \n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    '''Extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Create new columns for retweeted usernames, mentioned usernames and hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['retweeted'] = df.tweet.apply(find_retweeted)\n",
    "df['mentioned'] = df.tweet.apply(find_mentioned)\n",
    "df['hashtags'] = df.tweet.apply(find_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take the rows from the hashtag columns where there are actually hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list_df = df.loc[\n",
    "                       df.hashtags.apply(\n",
    "                           lambda hashtags_list: hashtags_list !=[]\n",
    "                       ),['hashtags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataframe where each use of a hashtag gets its own row.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_hashtags_df = pd.DataFrame(\n",
    "    [hashtag for hashtags_list in hashtags_list_df.hashtags\n",
    "    for hashtag in hashtags_list],\n",
    "    columns=['hashtag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "**Calculate number of unique hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_hashtags_df['hashtag'].unique().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count number of appearances for each hashtag.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_hashtags = flattened_hashtags_df.groupby('hashtag').size()\\\n",
    "                                        .reset_index(name='counts')\\\n",
    "                                        .sort_values('counts', ascending=False)\\\n",
    "                                        .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of times each hashtag appears.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = flattened_hashtags_df.groupby(['hashtag']).size()\\\n",
    "                              .reset_index(name='counts')\\\n",
    "                              .counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define bins for histogram of tweet counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bins = np.arange(0,counts.max()+2, 5)-0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Produce histogram of tweet counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(counts, bins = my_bins)\n",
    "plt.xlabels = np.arange(1,counts.max()+1, 1)\n",
    "plt.xlabel('Number of appearances for hashtags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get hashtags which appear at least twenty times.**\n",
    "**We'll consider these \"popular\" hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_appearance = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find popular hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_hashtags_set = set(popular_hashtags[\n",
    "                           popular_hashtags.counts >= min_appearance\n",
    "                           ]['hashtag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new column with only the popular hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list_df['popular_hashtags'] = hashtags_list_df.hashtags.apply(\n",
    "            lambda hashtag_list: [hashtag for hashtag in hashtag_list\n",
    "                                  if hashtag in popular_hashtags_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop rows which do not contain at least one popular hashtag.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_hashtags_list_df = hashtags_list_df.loc[\n",
    "            hashtags_list_df.popular_hashtags.apply( \\\n",
    "            lambda hashtag_list: hashtag_list !=[])]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new dataframe with the popular hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_vector_df = \\\n",
    "    popular_hashtags_list_df.loc[:, ['popular_hashtags']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create columns to record presence of hashtags.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hashtag in popular_hashtags_set:\n",
    "    hashtag_vector_df['{}'.format(hashtag)] = \\\n",
    "        hashtag_vector_df.popular_hashtags.apply(\n",
    "        lambda hashtag_list: int(hashtag in hashtag_list))\n",
    "\n",
    "hashtag_matrix = hashtag_vector_df.drop('popular_hashtags', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Calculate a hashtag correlation matrix.**\n",
    " **This tells us which hashtags tend to appear together.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = hashtag_matrix.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot correlation matrix.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.heatmap(correlations,\n",
    "    cmap='RdBu',\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    square = True,\n",
    "    cbar_kws={'label':'correlation'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define methods to clean tweets for further processing.**\n",
    "**We'll use nltk for natural language processing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Create a lemmatizer to fold inflected word forms together.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download( 'stopwords', quiet=True )\n",
    "nltk.download( 'wordnet', quiet=True )\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV\n",
    "           }\n",
    "\n",
    "# Get part of speech for a word. \n",
    "\n",
    "def get_wordnet_pos( word ):\n",
    "    tag = nltk.pos_tag( [word] )[0][1][0].upper()\n",
    "    return tag_dict.get( tag , wordnet.NOUN )\n",
    "\n",
    "# Remove web links from a tweet.\n",
    "\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "# Remove retweet and @user information from a tweet.\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    \n",
    "    # remove retweet\n",
    "    \n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    \n",
    "    # remove tweeted at\n",
    "\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)     \n",
    "    return tweet\n",
    "\n",
    "# Get the default English stopwords list from nltk.\n",
    "\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Get a list of punctuation to clean from tweet.\n",
    "\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean a tweet.**\n",
    "\n",
    "The resulting cleaned version of a tweet is placed in the column \"clean_tweet\" in the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, bigrams=False):\n",
    "    tweet = remove_users(tweet)  #remove users (handles)\n",
    "    tweet = remove_links(tweet)  # remove web links\n",
    "    tweet = tweet.lower()        # convert tweet to lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "\n",
    "    # tokenize the tweet.\n",
    "    \n",
    "    tweet_token_list = [word for word in tweet.strip().split(' ')\n",
    "                            if len(word) > 0 ]\n",
    "\n",
    "    # lemmatize the words in the tweet.\n",
    "    \n",
    "    tweet_token_list = [lemmatizer.lemmatize( word )\n",
    "        for word in tweet_token_list]\n",
    "\n",
    "    # remove stop words from lemma list.\n",
    "    tweet_token_list = [word for word in tweet_token_list\n",
    "                       if word not in my_stopwords]\n",
    "\n",
    "    # deal with bigrams if requested.\n",
    "    \n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "\n",
    "    # join the processed words in the tweet back to a string with a blank separating the words.\n",
    "\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "\n",
    "    # return the cleaned tweet.\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "df['clean_tweet'] = df.tweet.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use sci-kit learn to perform topic modeling on the cleaned tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create vectorizer object will to transform text to vector form.**\n",
    "**Ignore words that occur less than 25 times (min_df=25) or in more than**\n",
    "**90% of the documents (max_df=0.9).**\n",
    "\n",
    "**The token pattern indicates what a token looks like.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.9, min_df=25, \n",
    "                             token_pattern='\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply transformation to the cleaned tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = vectorizer.fit_transform(df['clean_tweet']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf_feature_names tells what word each column in the matrix represents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load both the Latent Dirichlet Allocation and Non-negative matrix factorization modules.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll extract ten topics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract topics using non-negative matrix factorization.**\n",
    "**NMF typically works better than LDA for short texts like tweets.**\n",
    "**As usual we initialize the extraction using a singular value decomposition**\n",
    "**(init='nndsvd') .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfModel = NMF( n_components=number_of_topics, init='nndsvd' )\n",
    "nmfModel.fit( tf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the top ten words in each topic.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "    \n",
    "num_top_words = 10\n",
    "\n",
    "print( \"Topics extracted using Non-negative matrix factorization:\" )\n",
    "\n",
    "display_topics( nmfModel, tf_feature_names, num_top_words ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment:  Compare NMF and LDA topics.\n",
    "\n",
    "**The following code extracts topics using Latent Dirichlet Allocation.**  **Compare the topics extracted by LDA with those extracted with NMF.**\n",
    "**Which appears to make more sense?  Or are they about equally** **interpretable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel = LatentDirichletAllocation( n_components=number_of_topics,\n",
    "    random_state=32767 )\n",
    "\n",
    "ldaModel.fit( tf )\n",
    "\n",
    "print( \"Topics extracted using Latent Dirichlet Allocation:\" )\n",
    "\n",
    "display_topics( ldaModel, tf_feature_names, num_top_words )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
